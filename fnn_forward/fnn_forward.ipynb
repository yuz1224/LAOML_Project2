{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `FeedforwardNeuralNetwork` class\n",
    "\n",
    "In the following code block, you can find a template for the class `FeedforwardNeuralNetwork` that implements a feedforward neural network\n",
    "$$\n",
    "\t\\begin{aligned}\n",
    "\t\t{\\rm NN}  & : \\mathbb{R}^{d} \\to \\mathbb{R}^{p}, \\\\\n",
    "\t\t{\\rm NN} (x,y) & = \\mathbf{A} \\cdot F_{L} \\circ \\ldots \\circ F_{1} (x,y)\n",
    "\t\\end{aligned}\n",
    "$$\n",
    "with \n",
    "$$\n",
    "\t\\begin{aligned}\n",
    "\t\tF_i : \\mathbb{R}^{n_{i-1}} & \\to \\mathbb{R}^{n_{i}}, \\\\\n",
    "\t\t\\mathbf{x}_{i} & = \\sigma\\left( \\mathbf{W}_i \\mathbf{x}_{i-1} + \\mathbf{b}_i \\right).\n",
    "\t\\end{aligned}\n",
    "$$\n",
    "for $i=1,\\ldots,L$. Here, $\\mathbf{W}_i \\in \\mathbb{R}^{n_{i} \\times n_{i-1}}$ ($n_0 \\coloneqq d$), $\\mathbf{b}_i \\in \\mathbb{R}^{n_{i}}$, $\\mathbf{A} \\in \\mathbb{R}^{p \\times n_L}$, and $\\sigma\\left(x\\right)$ is the activation function, which is applied element-wise.\n",
    "\n",
    "The implementations of the methods of this class are missing an have to be filled in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class FeedforwardNeuralNetwork:\n",
    "    def __init__(self, layer_sizes):\n",
    "        \"\"\"\n",
    "        Initialize the feedforward neural network.\n",
    "        \n",
    "        Parameters:\n",
    "        layer_sizes (list): List containing the number of neurons in each layer.\n",
    "        \"\"\"\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.num_layers = len(layer_sizes)\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "\n",
    "        for i in range(1, len(self.layer_sizes)):\n",
    "            self.weights.append(np.random.randn(self.layer_sizes[i], self.layer_sizes[i - 1]) * 0.01)\n",
    "            self.biases.append(np.zeros((self.layer_sizes[i], 1)))\n",
    "            \n",
    "            \n",
    "            \n",
    "    def activation(self, z):\n",
    "        \"\"\"\n",
    "        Activation function.\n",
    "        \n",
    "        Parameters:\n",
    "        z (numpy.ndarray): Input array.\n",
    "        \n",
    "        Returns:\n",
    "        numpy.ndarray: Output array after applying the activation function.\n",
    "        \"\"\"\n",
    "        \n",
    "        return np.maximum(0, z) # ReLU\n",
    "        # tanh return np.tanh(z) \n",
    "\n",
    "\n",
    "    def activation_derivative(self, z):\n",
    "        \"\"\"\n",
    "        Derivative of the activation function.\n",
    "        \n",
    "        Parameters:\n",
    "        z (numpy.ndarray): Input array.\n",
    "        \n",
    "        Returns:\n",
    "        numpy.ndarray: Output array after applying derivative of the activation function.\n",
    "        \"\"\"\n",
    "        \n",
    "        return (z > 0).astype(float)\n",
    "        # tanh return 1 - np.tanh(z) ** 2\n",
    "    \n",
    "    def feedforward(self, x):\n",
    "        \"\"\"\n",
    "        Perform a feedforward pass through the network.\n",
    "        \n",
    "        Parameters:\n",
    "        x (numpy.ndarray): Input array.\n",
    "        \n",
    "        Returns:\n",
    "        numpy.ndarray: Output of the network.\n",
    "        \"\"\"\n",
    "            \n",
    "        activations = [x]  \n",
    "        zs = [] \n",
    "        a = x\n",
    "        \n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(w, a) + b\n",
    "            zs.append(z)\n",
    "            a = self.activation(z)\n",
    "            activations.append(a)\n",
    "        return activations, zs\n",
    "\n",
    "\n",
    "    def compute_cost(self, y_pred, y_train):\n",
    "        \"\"\"\n",
    "        Compute the cost function.\n",
    "        \n",
    "        Parameters:\n",
    "        y_pred (numpy.ndarray): Predicted labels.\n",
    "        y_train (numpy.ndarray): True labels.\n",
    "        \n",
    "        Returns:\n",
    "        float: Cost value.\n",
    "        \"\"\"\n",
    "\n",
    "        return np.mean((y_pred - y_train)**2)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Activation Function:\n",
      "    The activation function introduces non-linearity, enabling the network to learn complex patterns and relationships in data.\n",
      "    1. Default: ReLU (Rectified Linear Unit):\n",
      "        Mathematical expression:\n",
      "            𝑓(𝑧) = max(0,𝑧)\n",
      "    2. Optional: tanh (Hyperbolic Tangent):\n",
      "        Mathematical expression:\n",
      "            𝑓(𝑧) = tanh(𝑧)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "activation_function_text = \"\"\"\n",
    "Activation Function:\n",
    "    The activation function introduces non-linearity, enabling the network to learn complex patterns and relationships in data.\n",
    "    1. Default: ReLU (Rectified Linear Unit):\n",
    "        Mathematical expression:\n",
    "            𝑓(𝑧) = max(0,𝑧)\n",
    "    2. Optional: tanh (Hyperbolic Tangent):\n",
    "        Mathematical expression:\n",
    "            𝑓(𝑧) = tanh(𝑧)\n",
    "\"\"\"\n",
    "\n",
    "print(activation_function_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(self, z):\n",
    "        \"\"\"\n",
    "        Activation function.\n",
    "        \n",
    "        Parameters:\n",
    "        z (numpy.ndarray): Input array.\n",
    "        \n",
    "        Returns:\n",
    "        numpy.ndarray: Output array after applying the activation function.\n",
    "        \"\"\"\n",
    "        \n",
    "        return np.maximum(0, z) # ReLU\n",
    "        # tanh return np.tanh(z) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivative_activation_function_text = \"\"\"\n",
    "Derivative of the Activation Function:\n",
    "    Used during backpropagation to compute gradients.\n",
    "    1. ReLU Derivative:\n",
    "        Mathematical expression:\n",
    "            𝑓'(𝑧) = {\n",
    "                1 if 𝑧 > 0,\n",
    "                0 if 𝑧 ≤ 0\n",
    "            }\n",
    "    2. tanh Derivative:\n",
    "        Mathematical expression:\n",
    "            𝑓'(𝑧) = 1 − tanh²(𝑧)\n",
    "\"\"\"\n",
    "\n",
    "print(derivative_activation_function_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   def activation_derivative(self, z):\n",
    "        \"\"\"\n",
    "        Derivative of the activation function.\n",
    "        \n",
    "        Parameters:\n",
    "        z (numpy.ndarray): Input array.\n",
    "        \n",
    "        Returns:\n",
    "        numpy.ndarray: Output array after applying derivative of the activation function.\n",
    "        \"\"\"\n",
    "        \n",
    "        return (z > 0).astype(float)\n",
    "        # tanh return 1 - np.tanh(z) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss Function:\n",
      "    The loss function quantifies the difference between the network’s predicted outputs (𝑦_pred) and the true values (𝑦_train).\n",
      "    1. Mean Squared Error (MSE):\n",
      "        Mathematical expression:\n",
      "            MSE = (1 / 𝑛) * Σ(i=1 to 𝑛) (𝑦_pred,𝑖 − 𝑦_train,𝑖)²\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss_function_text = \"\"\"\n",
    "Loss Function:\n",
    "    The loss function quantifies the difference between the network’s predicted outputs (𝑦_pred) and the true values (𝑦_train).\n",
    "    1. Mean Squared Error (MSE):\n",
    "        Mathematical expression:\n",
    "            MSE = (1 / 𝑛) * Σ(i=1 to 𝑛) (𝑦_pred,𝑖 − 𝑦_train,𝑖)²\n",
    "\"\"\"\n",
    "\n",
    "print(loss_function_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def compute_cost(self, y_pred, y_train):\n",
    "        \"\"\"\n",
    "        Compute the cost function.\n",
    "        \n",
    "        Parameters:\n",
    "        y_pred (numpy.ndarray): Predicted labels.\n",
    "        y_train (numpy.ndarray): True labels.\n",
    "        \n",
    "        Returns:\n",
    "        float: Cost value.\n",
    "        \"\"\"\n",
    "\n",
    "        return np.mean((y_pred - y_train)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Forward Propagation:\n",
      "        Forward propagation passes input data through each layer of the network to generate output predictions.\n",
      "        1. Layer-wise Computation:\n",
      "            - Linear transformation:\n",
      "                𝑧[𝑙] = 𝑊[𝑙]𝑎[𝑙−1] + 𝑏[𝑙]\n",
      "            - Activation function:\n",
      "                𝑎[𝑙] = 𝑓(𝑧[𝑙])\n",
      "        2. Intermediate Storage:\n",
      "            - During forward propagation, both the activations (𝑎[𝑙]) and linear transformations (𝑧[𝑙]) \n",
      "              are stored for each layer. These values are later used during backpropagation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "forward_propagation_text = \"\"\"\n",
    "Forward Propagation:\n",
    "        Forward propagation passes input data through each layer of the network to generate output predictions.\n",
    "        1. Layer-wise Computation:\n",
    "            - Linear transformation:\n",
    "                𝑧[𝑙] = 𝑊[𝑙]𝑎[𝑙−1] + 𝑏[𝑙]\n",
    "            - Activation function:\n",
    "                𝑎[𝑙] = 𝑓(𝑧[𝑙])\n",
    "        2. Intermediate Storage:\n",
    "            - During forward propagation, both the activations (𝑎[𝑙]) and linear transformations (𝑧[𝑙]) \n",
    "              are stored for each layer. These values are later used during backpropagation.\n",
    "\"\"\"\n",
    "\n",
    "print(forward_propagation_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def feedforward(self, x):\n",
    "        \"\"\"\n",
    "        Perform a feedforward pass through the network.\n",
    "        \n",
    "        Parameters:\n",
    "        x (numpy.ndarray): Input array.\n",
    "        \n",
    "        Returns:\n",
    "        numpy.ndarray: Output of the network.\n",
    "        \"\"\"\n",
    "            \n",
    "        activations = [x]   # Store activations for all layers\n",
    "        linear_outputs = []  # Store pre-activation values for all layers\n",
    "        a = x  # Input is the activation of the first layer\n",
    "        \n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(w, a) + b  # Linear transformation\n",
    "            linear_outputs.append(z)\n",
    "            a = self.activation(z)  # Apply activation function\n",
    "            activations.append(a)\n",
    "        return activations, linear_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
