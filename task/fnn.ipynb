{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `FeedforwardNeuralNetwork` class\n",
    "\n",
    "In the following code block, you can find a template for the class `FeedforwardNeuralNetwork` that implements a feedforward neural network\n",
    "$$\n",
    "\t\\begin{aligned}\n",
    "\t\t{\\rm NN}  & : \\mathbb{R}^{d} \\to \\mathbb{R}^{p}, \\\\\n",
    "\t\t{\\rm NN} (x,y) & = \\mathbf{A} \\cdot F_{L} \\circ \\ldots \\circ F_{1} (x,y)\n",
    "\t\\end{aligned}\n",
    "$$\n",
    "with \n",
    "$$\n",
    "\t\\begin{aligned}\n",
    "\t\tF_i : \\mathbb{R}^{n_{i-1}} & \\to \\mathbb{R}^{n_{i}}, \\\\\n",
    "\t\t\\mathbf{x}_{i} & = \\sigma\\left( \\mathbf{W}_i \\mathbf{x}_{i-1} + \\mathbf{b}_i \\right).\n",
    "\t\\end{aligned}\n",
    "$$\n",
    "for $i=1,\\ldots,L$. Here, $\\mathbf{W}_i \\in \\mathbb{R}^{n_{i} \\times n_{i-1}}$ ($n_0 \\coloneqq d$), $\\mathbf{b}_i \\in \\mathbb{R}^{n_{i}}$, $\\mathbf{A} \\in \\mathbb{R}^{p \\times n_L}$, and $\\sigma\\left(x\\right)$ is the activation function, which is applied element-wise.\n",
    "\n",
    "The implementations of the methods of this class are missing an have to be filled in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class FeedforwardNeuralNetwork:\n",
    "    def __init__(self, layer_sizes, activation_type = \"relu\"):\n",
    "        \"\"\"\n",
    "        Initialize the feedforward neural network.\n",
    "        \n",
    "        Parameters:\n",
    "        layer_sizes (list): List containing the number of neurons in each layer, including the layer size for input and output layers.\n",
    "        \"\"\"\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.num_layers = len(layer_sizes)\n",
    "        if activation_type in [\"relu\", \"sigmoid\", \"tanh\"]:\n",
    "            self.activation_type = activation_type\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid activation function '{self.activation_type}'. Supported types: relu, sigmoid, tanh.\")\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        # initialize all weights with N(0,1) * 0.01\n",
    "        # Initialize all biases with 0\n",
    "        for i in range(1, len(self.layer_sizes)):\n",
    "            self.weights.append(np.random.randn(self.layer_sizes[i], self.layer_sizes[i - 1]) * 0.01)\n",
    "            self.biases.append(np.zeros((self.layer_sizes[i], 1)))\n",
    "\n",
    "    def activation(self, z):\n",
    "        \"\"\"\n",
    "        Activation function.\n",
    "        \n",
    "        Parameters:\n",
    "        z (numpy.ndarray): Input array.\n",
    "        \n",
    "        Returns:\n",
    "        numpy.ndarray: Output array after applying the activation function.\n",
    "        \"\"\"\n",
    "        \n",
    "        # using relu function here\n",
    "        if self.activation_type == \"relu\":\n",
    "            return np.maximum(0, z)\n",
    "        elif self.activation_type == \"sigmoid\":\n",
    "            return 1 / (1 + np.exp(-z))\n",
    "        elif self.activation_type == \"tanh\":\n",
    "            return np.tanh(z)\n",
    "        else:\n",
    "            raise Exception(\"Invalid activation function\")\n",
    "\n",
    "    def activation_derivative(self, z):\n",
    "        \"\"\"\n",
    "        Derivative of the activation function.\n",
    "        \n",
    "        Parameters:\n",
    "        z (numpy.ndarray): Input array.\n",
    "        \n",
    "        Returns:\n",
    "        numpy.ndarray: Output array after applying derivative of the activation function.\n",
    "        \"\"\"\n",
    "        if self.activation_type == \"relu\":\n",
    "            return (z>=0).astype(float)\n",
    "        elif self.activation_type == \"sigmoid\":\n",
    "            sigmoid = self.activation(z)\n",
    "            return sigmoid * (1 - sigmoid)\n",
    "        elif self.activation_type == \"tanh\":\n",
    "            return 1 - self.activation(z)**2\n",
    "        else:\n",
    "            raise Exception(\"Invalid activation function\")\n",
    "\n",
    "    def feedforward(self, x):\n",
    "        \"\"\"\n",
    "        Perform a feedforward pass through the network.\n",
    "        \n",
    "        Parameters:\n",
    "        x (numpy.ndarray): Input array.\n",
    "        \n",
    "        Returns:\n",
    "        numpy.ndarray: Output of the network.\n",
    "        \"\"\"\n",
    "        a_values = [x] # values after activation functions\n",
    "        z_values = [] # values before activation functions\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(w, a_values[-1]) + b\n",
    "            z_values.append(z)\n",
    "            a = self.activation(z) if len(z_values) < self.num_layers - 1 else z\n",
    "            a_values.append(a)\n",
    "        return a_values, z_values\n",
    "\n",
    "    def compute_cost(self, y_pred, y_train):\n",
    "        \"\"\"\n",
    "        Compute the cost function.\n",
    "        \n",
    "        Parameters:\n",
    "        y_pred (numpy.ndarray): Predicted labels.\n",
    "        y_train (numpy.ndarray): True labels.\n",
    "        \n",
    "        Returns:\n",
    "        float: Cost value.\n",
    "        \"\"\"\n",
    "        # Using MSE Loss Function here\n",
    "        return np.mean((y_train - y_pred)**2)\n",
    "    \n",
    "    def backpropagate(self, z_values, a_values, y_true):\n",
    "        \"\"\"\n",
    "        Perform backpropagation to compute gradients.\n",
    "        \n",
    "        Parameters:\n",
    "        z_values (list): outputs before activation functions in each layer.\n",
    "        a_values (list): outputs after activation functions in each layer.\n",
    "        y_true (numpy.ndarray): True labels.\n",
    "        \n",
    "        Returns:\n",
    "        tuple: Gradients of weights and biases.\n",
    "        \"\"\"\n",
    "        \n",
    "        # initialize the gradient for a, w and b\n",
    "        w_gradient = []\n",
    "        b_gradient = []\n",
    "        \n",
    "        # no activation function for output layers\n",
    "        # compute the loss for the output layer individually\n",
    "        # let error be np.mean(y_pred - y) \n",
    "        z_gradient = 2 * (a_values[-1] - y_true)  # shape = {1, batch_size}\n",
    "        w_gradient.append(z_gradient @ a_values[-2].T) # shape = {1, a_{L-1}}\n",
    "        b_gradient.append(np.sum(z_gradient), axis = -1) # shape = {1, 1}\n",
    "        \n",
    "        for l in range(2, self.num_layers):\n",
    "            \n",
    "            # compute the gradient of loss w.r.t z^{-l} first\n",
    "            ## l start from the 2nd to last layer \n",
    "            ## we have dz^{-l} = W^{a_{-l+1}, a^{-l}}^T @ (derivative of activation * dz^{-l+1})\n",
    "            \n",
    "            ## the derivative of activation function\n",
    "            # # derivative of a^{-l} w.r.t z^{-l} \n",
    "            act_de = self.activation_derivative(z_values[-l]) # shape = {a_{-l}, batch_size}\n",
    "            \n",
    "            ## the derivative of loss w.r.t z^{-l} \n",
    "            z_gradient = self.weights[-l + 1].T @ (act_de * z_gradient) # shape = {a_{-l}, batch_size}\n",
    "            \n",
    "            # the derivative of loss w.r.t w^{a^{-l}, a^{l-1}}\n",
    "            w_gradient.append(z_gradient @ a_values[-l - 1].T) # shape = {a_{-l}, a_{-l - 1}}\n",
    "            \n",
    "            # the derivative of loss w.r.t b^{a^{-l}}\n",
    "            b_gradient.append(np.sum(z_gradient), axis = -1) # shape = {a_{-l}, 1}\n",
    "            \n",
    "        return w_gradient[::-1], b_gradient[::-1]\n",
    "\n",
    "    def update_parameters(self, nabla_w, nabla_b, learning_rate, opt_type = \"sgd\"):\n",
    "        \"\"\"\n",
    "        Update the weights and biases using the computed gradients.\n",
    "        \n",
    "        Parameters:\n",
    "        nabla_w (list): Gradients of weights.\n",
    "        nabla_b (list): Gradients of biases.\n",
    "        learning_rate (float): Learning rate.\n",
    "        \"\"\"\n",
    "        \n",
    "        if hasattr(self, \"adm_init\"):\n",
    "            self.adm_init = True\n",
    "            self.w_f = [np.zeros_like(w) for w in self.weights] # 1st order moment for weights \n",
    "            self.w_s = [np.zeros_like(w) for w in self.weights] # 2nd order moment for weights\n",
    "            \n",
    "            self.b_f = [np.zeros_like(b) for b in self.biases] # 1st order moment for biases\n",
    "            self.b_s = [np.zeros_like(b) for b in self.biases] # 2nd order moment for biases\n",
    "            \n",
    "        if opt_type == \"gd\":\n",
    "            self.weights = [w - learning_rate * dw for w, dw in zip(self.weights, nabla_w)]\n",
    "            self.biases = [b - learning_rate * db for b, db in zip(self.biases, nabla_b)]\n",
    "        elif opt_type == \"adam\":\n",
    "            for i in range(len(nabla_w)):\n",
    "                self.b_f[i] \n",
    "            \n",
    "                \n",
    "                \n",
    "                \n",
    "            self.weights = [ for w in self.weights]\n",
    "            self.biases = [ for b in self.biases]\n",
    "            \n",
    "        else:\n",
    "            raise Exception(f\"Invalid optimization type {opt_type}. Supported types: \\\"sgd\\\", \\\"adam\\\".\")\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    def train(self, x_train, y_train, epochs, learning_rate, batch_size):\n",
    "        \"\"\"\n",
    "        Train the neural network using mini-batch gradient descent with early stopping.\n",
    "        \n",
    "        Parameters:\n",
    "        x_train (numpy.ndarray): Training data.\n",
    "        y_train (numpy.ndarray): Training labels.\n",
    "        epochs (int): Number of epochs.\n",
    "        learning_rate (float): Learning rate.\n",
    "        batch_size (int): Size of each mini-batch.\n",
    "        \"\"\"\n",
    "        np.random.seed(42)\n",
    "        num_samples = x_train.shape[0]\n",
    "        for _ in range(epochs):\n",
    "            idx = np.random.permutation(num_samples)\n",
    "            x_train = x_train[idx]\n",
    "            y_train = y_train[idx]\n",
    "            for i in range(np.ceil(num_samples / batch_size)):\n",
    "                if (i+1) * batch_size >= num_samples:\n",
    "                    x_train = x_train[i * batch_size:]\n",
    "                    y_train = y_train[i * batch_size:]\n",
    "                else:\n",
    "                    x_batch = x_train[i * batch_size: (i + 1) * batch_size]\n",
    "                    y_batch = y_train[i * batch_size: (i + 1) * batch_size]\n",
    "                \n",
    "                # Forward & Backward Propagation\n",
    "                # self.feedforword method is called at self.backpropagate\n",
    "                \n",
    "            \n",
    "            \n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
