{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `FeedforwardNeuralNetwork` class\n",
    "\n",
    "In the following code block, you can find a template for the class `FeedforwardNeuralNetwork` that implements a feedforward neural network\n",
    "$$\n",
    "\t\\begin{aligned}\n",
    "\t\t{\\rm NN}  & : \\mathbb{R}^{d} \\to \\mathbb{R}^{p}, \\\\\n",
    "\t\t{\\rm NN} (x,y) & = \\mathbf{A} \\cdot F_{L} \\circ \\ldots \\circ F_{1} (x,y)\n",
    "\t\\end{aligned}\n",
    "$$\n",
    "with \n",
    "$$\n",
    "\t\\begin{aligned}\n",
    "\t\tF_i : \\mathbb{R}^{n_{i-1}} & \\to \\mathbb{R}^{n_{i}}, \\\\\n",
    "\t\t\\mathbf{x}_{i} & = \\sigma\\left( \\mathbf{W}_i \\mathbf{x}_{i-1} + \\mathbf{b}_i \\right).\n",
    "\t\\end{aligned}\n",
    "$$\n",
    "for $i=1,\\ldots,L$. Here, $\\mathbf{W}_i \\in \\mathbb{R}^{n_{i} \\times n_{i-1}}$ ($n_0 \\coloneqq d$), $\\mathbf{b}_i \\in \\mathbb{R}^{n_{i}}$, $\\mathbf{A} \\in \\mathbb{R}^{p \\times n_L}$, and $\\sigma\\left(x\\right)$ is the activation function, which is applied element-wise.\n",
    "\n",
    "The implementations of the methods of this class are missing an have to be filled in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```The following packages are used in this project```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 1 – Feedforward Neural Network (FNN)**\n",
    "##### **a) Implement an FNN for approximating the solutions of problem (Poisson 2D) with**\n",
    "$$\n",
    "    \\begin{aligned}\n",
    "\t\t- \\Delta u (x,y) & = 2 \\pi^2 \\sin (\\pi x) \\sin (\\pi y) & & \\text{in } \\Omega, \\\\\n",
    "\t\tu(x,y) & = 0 & & \\text{on } \\partial\\Omega, \n",
    "\t\\end{aligned}\n",
    "$$\n",
    "**for $\\Omega = [0,1]^2$ using data. You can generate the training data yourself by choosing points $(x_i, y_i)$ in $\\Omega$ as input and evaluating the analytical solution $u(x_i, y_i)$ in these points as output. You may decide how to choose the points yourself $u(x_i, y_i)$. It is advised to normalize the data, for instance, by transforming them to the interval $[0, 1]$ (min-max scaling) before training the neural network to fit the data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we use the following code to generate the training data, returns the input data $(x_i, y_i)$ and output data $u(x_i, y_i)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generation for the Poisson equation\n",
    "def generate_data(num_points=99):\n",
    "    def analytical_solution(x, y):\n",
    "        # Poisson equation analytical solution\n",
    "        sol = np.sin(np.pi * x) * np.sin(np.pi * y)\n",
    "        sol[(x == 1) | (y == 1)] = 0\n",
    "        return sol\n",
    "    x = np.linspace(0, 1, num_points)\n",
    "    y = np.linspace(0, 1, num_points)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    u = analytical_solution(X, Y)\n",
    "    return np.c_[X.ravel(), Y.ravel()], u.ravel()\n",
    "\n",
    "# Initialize data\n",
    "input_data, output_data = generate_data(num_points=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For your implementation implement the missing functions in the template for the class `FeedforwardNeuralNetwork` to be found in the Jupyter notebook `fnn.ipynb` in the assignment\n",
    "on Brightspace. Most importantly, this includes\n",
    "\n",
    "- the activation function,\n",
    "- the loss function,\n",
    "- the forward propagation,\n",
    "- the backward propagation,\n",
    "- the gradient descent algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sympy.abc import epsilon\n",
    "\n",
    "\n",
    "class FeedforwardNeuralNetwork:\n",
    "    def __init__(self, layer_sizes, activation_type = \"relu\"):\n",
    "        \"\"\"\n",
    "        Initialize the feedforward neural network.\n",
    "        \n",
    "        Parameters:\n",
    "        layer_sizes (list): List containing the number of neurons in each layer, including the layer size for input and output layers.\n",
    "        \"\"\"\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.num_layers = len(layer_sizes)\n",
    "        if activation_type in [\"relu\", \"sigmoid\", \"tanh\"]:\n",
    "            self.activation_type = activation_type\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid activation function '{self.activation_type}'. Supported types: relu, sigmoid, tanh.\")\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        # initialize all weights with N(0,1) * 0.01\n",
    "        # Initialize all biases with 0\n",
    "        for i in range(1, len(self.layer_sizes)):\n",
    "            self.weights.append(np.random.randn(self.layer_sizes[i], self.layer_sizes[i - 1]) * 0.01)\n",
    "            self.biases.append(np.zeros((self.layer_sizes[i], 1)))\n",
    "\n",
    "    def activation(self, z):\n",
    "        \"\"\"\n",
    "        Activation function.\n",
    "        \n",
    "        Parameters:\n",
    "        z (numpy.ndarray): Input array.\n",
    "        \n",
    "        Returns:\n",
    "        numpy.ndarray: Output array after applying the activation function.\n",
    "        \"\"\"\n",
    "        \n",
    "        # using relu function here\n",
    "        if self.activation_type == \"relu\":\n",
    "            return np.maximum(0, z)\n",
    "        elif self.activation_type == \"sigmoid\":\n",
    "            return 1 / (1 + np.exp(-z))\n",
    "        elif self.activation_type == \"tanh\":\n",
    "            return np.tanh(z)\n",
    "        else:\n",
    "            raise Exception(\"Invalid activation function\")\n",
    "\n",
    "    def activation_derivative(self, z):\n",
    "        \"\"\"\n",
    "        Derivative of the activation function.\n",
    "        \n",
    "        Parameters:\n",
    "        z (numpy.ndarray): Input array.\n",
    "        \n",
    "        Returns:\n",
    "        numpy.ndarray: Output array after applying derivative of the activation function.\n",
    "        \"\"\"\n",
    "        if self.activation_type == \"relu\":\n",
    "            return (z > 0).astype(float)\n",
    "        elif self.activation_type == \"sigmoid\":\n",
    "            sigmoid = self.activation(z)\n",
    "            return sigmoid * (1 - sigmoid)\n",
    "        elif self.activation_type == \"tanh\":\n",
    "            return 1 - self.activation(z)**2\n",
    "        else:\n",
    "            raise Exception(\"Invalid activation function\")\n",
    "\n",
    "    def feedforward(self, x):\n",
    "        \"\"\"\n",
    "        Perform a feedforward pass through the network.\n",
    "        \n",
    "        Parameters:\n",
    "        x (numpy.ndarray): Input array.\n",
    "        \n",
    "        Returns:\n",
    "        numpy.ndarray: Output of the network.\n",
    "        \"\"\"\n",
    "        a_values = [x] # values after activation functions\n",
    "        z_values = [] # values before activation functions\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(w, a_values[-1]) + b\n",
    "            z_values.append(z)\n",
    "            a = self.activation(z) if len(z_values) < self.num_layers - 1 else z\n",
    "            a_values.append(a)\n",
    "        return a_values, z_values\n",
    "\n",
    "    def compute_cost(self, y_pred, y_train):\n",
    "        \"\"\"\n",
    "        Compute the cost function.\n",
    "        \n",
    "        Parameters:\n",
    "        y_pred (numpy.ndarray): Predicted labels.\n",
    "        y_train (numpy.ndarray): True labels.\n",
    "        \n",
    "        Returns:\n",
    "        float: Cost value.\n",
    "        \"\"\"\n",
    "        # Using MSE Loss Function here\n",
    "        return np.mean((y_train - y_pred)**2)\n",
    "    \n",
    "    def backpropagate(self, z_values, a_values, y_true):\n",
    "        \"\"\"\n",
    "        Perform backpropagation to compute gradients.\n",
    "        \n",
    "        Parameters:\n",
    "        z_values (list): outputs before activation functions in each layer.\n",
    "        a_values (list): outputs after activation functions in each layer.\n",
    "        y_true (numpy.ndarray): True labels.\n",
    "        \n",
    "        Returns:\n",
    "        tuple: Gradients of weights and biases.\n",
    "        \"\"\"\n",
    "        \n",
    "        # initialize the gradient for a, w and b\n",
    "        w_gradient = []\n",
    "        b_gradient = []\n",
    "        \n",
    "        # no activation function for output layers\n",
    "        # compute the loss for the output layer individually\n",
    "        # let error be np.mean(y_pred - y) \n",
    "        z_gradient = 2 * (a_values[-1] - y_true)  # shape = {1, batch_size}\n",
    "        w_gradient.append(z_gradient @ a_values[-2].T) # shape = {1, a_{L-1}}\n",
    "        b_gradient.append(np.sum(z_gradient, axis = -1)) # shape = {1, 1}\n",
    "        \n",
    "        for l in range(2, self.num_layers):\n",
    "            \n",
    "            # compute the gradient of loss w.r.t z^{-l} first\n",
    "            ## l start from the 2nd to last layer \n",
    "            ## we have dz^{-l} = W^{a_{-l+1}, a^{-l}}^T @ (derivative of activation * dz^{-l+1})\n",
    "            \n",
    "            ## the derivative of activation function\n",
    "            # # derivative of a^{-l} w.r.t z^{-l} \n",
    "            act_de = self.activation_derivative(z_values[-l]) # shape = {a_{-l}, batch_size}\n",
    "            \n",
    "            ## the derivative of loss w.r.t z^{-l} \n",
    "            z_gradient = self.weights[-l + 1].T @ (act_de * z_gradient) # shape = {a_{-l}, batch_size}\n",
    "            \n",
    "            # the derivative of loss w.r.t w^{a^{-l}, a^{l-1}}\n",
    "            w_gradient.append(z_gradient @ a_values[-l - 1].T) # shape = {a_{-l}, a_{-l - 1}}\n",
    "            \n",
    "            # the derivative of loss w.r.t b^{a^{-l}}\n",
    "            b_gradient.append(np.sum(z_gradient), axis = -1) # shape = {a_{-l}, 1}\n",
    "            \n",
    "        return w_gradient[::-1], b_gradient[::-1]\n",
    "\n",
    "    def update_parameters(self, nabla_w, nabla_b, learning_rate, opt_type = \"sgd\"):\n",
    "        \"\"\"\n",
    "        Update the weights and biases using the computed gradients.\n",
    "        \n",
    "        Parameters:\n",
    "        nabla_w (list): Gradients of weights.\n",
    "        nabla_b (list): Gradients of biases.\n",
    "        learning_rate (float): Learning rate.\n",
    "        \"\"\"\n",
    "        if opt_type == \"gd\":\n",
    "            self.weights = [w - learning_rate * dw for w, dw in zip(self.weights, nabla_w)]\n",
    "            self.biases = [b - learning_rate * db for b, db in zip(self.biases, nabla_b)]\n",
    "        elif opt_type == \"adam\":\n",
    "            if not hasattr(self, \"adm_step\"):\n",
    "                self.adm_step = 1\n",
    "                self.w_f = [np.zeros_like(w) for w in self.weights] # 1st order moment for weights \n",
    "                self.w_s = [np.zeros_like(w) for w in self.weights] # 2nd order moment for weights\n",
    "                \n",
    "                self.b_f = [np.zeros_like(b) for b in self.biases] # 1st order moment for biases\n",
    "                self.b_s = [np.zeros_like(b) for b in self.biases] # 2nd order moment for biases\n",
    "            \n",
    "            # initial parameters\n",
    "            eps = 1e-10\n",
    "            beta1 = 0.9\n",
    "            beta2 = 0.999\n",
    "            factor1 = 1 - beta1**self.adm_step\n",
    "            factor2 = 1 - beta2**self.adm_step\n",
    "            \n",
    "            for i in range(len(nabla_w)):\n",
    "                self.b_f[i] = (beta1 * self.b_f[i] + (1-beta1) * nabla_b[i])\n",
    "                self.w_f[i] = (beta1 * self.w_f[i] + (1-beta1) * nabla_w[i])\n",
    "                \n",
    "                self.b_s[i] = (beta2 * self.b_s[i] + (1-beta2) * nabla_b[i])\n",
    "                self.w_s[i] = (beta2 * self.w_s[i] + (1-beta2) * nabla_w[i])\n",
    "                \n",
    "                b_f_hat = self.b_f[i]/factor1\n",
    "                w_f_hat = self.w_f[i]/factor1\n",
    "                \n",
    "                b_s_hat = self.b_s[i]/factor2\n",
    "                w_s_hat = self.w_s[i]/factor2\n",
    "                \n",
    "                self.weights[i] -= w_f_hat / (eps + np.sqrt(w_s_hat))\n",
    "                self.biases[i] -= b_f_hat / (eps + np.sqrt(b_s_hat))\n",
    "                \n",
    "            self.adm_step +=1\n",
    "        else:\n",
    "            raise Exception(f\"Invalid optimization type {opt_type}. Supported types: \\\"sgd\\\", \\\"adam\\\".\")\n",
    "        \n",
    "    def train(self, x_train, y_train, epochs, learning_rate, batch_size, opt_type = \"adm\"):\n",
    "        \"\"\"\n",
    "        Train the neural network using mini-batch gradient descent with early stopping.\n",
    "        \n",
    "        Parameters:\n",
    "        x_train (numpy.ndarray): Training data.\n",
    "        y_train (numpy.ndarray): Training labels.\n",
    "        epochs (int): Number of epochs.\n",
    "        learning_rate (float): Learning rate.\n",
    "        batch_size (int): Size of each mini-batch.\n",
    "        \"\"\"\n",
    "        np.random.seed(42)\n",
    "        num_samples = x_train.shape[0]\n",
    "        for _ in range(epochs):\n",
    "            idx = np.random.permutation(num_samples)\n",
    "            x_train = x_train[:,idx]\n",
    "            y_train = y_train[:,idx] # 如果y是一维的变量，有可能batch对应的dim不在第二个\n",
    "            for i in range(np.ceil(num_samples / batch_size)):\n",
    "                if (i+1) * batch_size >= num_samples:\n",
    "                    x_batch = x_train[:,i * batch_size:]\n",
    "                    y_batch = y_train[:,i * batch_size:]\n",
    "                else:\n",
    "                    x_batch = x_train[:,i * batch_size: (i + 1) * batch_size]\n",
    "                    y_batch = y_train[:,i * batch_size: (i + 1) * batch_size]\n",
    "                \n",
    "                # Forward & Backward Propagation\n",
    "                a_values, z_values = self.feedforward(x_batch)\n",
    "                nabla_w, nabla_b = self.backpropagate(z_values, a_values, y_batch)\n",
    "                self.update_parameters(nabla_w, nabla_b, learning_rate, opt_type)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
